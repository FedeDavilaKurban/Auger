{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determine when a sampel is to sparse to use patch-based resampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision:\n",
      "patch_jackknife chosen 100\n",
      "Warnings (sample): [(200, ['min_per_patch=10<20'])]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Updated patch-selection and resampling decision toolkit for TreeCorr\n",
    "Incorporates CV as a *sanity check only* and uses physics-driven criteria:\n",
    " - min objects per patch\n",
    " - min pair counts per patch (n*(n-1)/2)\n",
    " - fraction of empty patches\n",
    " - Kish effective N\n",
    " - patch angular radius relative to max correlation scale\n",
    "\n",
    "Usage:\n",
    " - import functions from this file\n",
    " - call choose_patches_and_resampling(...) with your RA/Dec arrays and desired parameters\n",
    " - labels for chosen npatch are returned and can be used to build jackknife folds\n",
    "\n",
    "Author: Nova (assistant)\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from collections import defaultdict\n",
    "import math\n",
    "\n",
    "# Optional: try to import healpy for healpix-based patching\n",
    "try:\n",
    "    import healpy as hp\n",
    "    HAS_HEALPY = True\n",
    "except Exception:\n",
    "    HAS_HEALPY = False\n",
    "\n",
    "# ----------------------------- Utilities -----------------------------\n",
    "\n",
    "def sphere_to_cartesian(ra_deg, dec_deg):\n",
    "    ra = np.deg2rad(ra_deg)\n",
    "    dec = np.deg2rad(dec_deg)\n",
    "    x = np.cos(dec) * np.cos(ra)\n",
    "    y = np.cos(dec) * np.sin(ra)\n",
    "    z = np.sin(dec)\n",
    "    return np.vstack([x, y, z]).T\n",
    "\n",
    "\n",
    "def angular_separation_deg(ra1_deg, dec1_deg, ra2_deg, dec2_deg):\n",
    "    \"\"\"Accurate angular separation in degrees between two points.\"\"\"\n",
    "    ra1, dec1, ra2, dec2 = map(np.deg2rad, (ra1_deg, dec1_deg, ra2_deg, dec2_deg))\n",
    "    sin_d1, cos_d1 = np.sin(dec1), np.cos(dec1)\n",
    "    sin_d2, cos_d2 = np.sin(dec2), np.cos(dec2)\n",
    "    cos_dphi = np.cos(ra1 - ra2)\n",
    "    cos_theta = sin_d1*sin_d2 + cos_d1*cos_d2*cos_dphi\n",
    "    cos_theta = np.clip(cos_theta, -1.0, 1.0)\n",
    "    return np.degrees(np.arccos(cos_theta))\n",
    "\n",
    "# ----------------------------- Patching -----------------------------\n",
    "\n",
    "def make_patches_kmeans(ra_deg, dec_deg, npatch, random_state=0):\n",
    "    \"\"\"k-means clustering on 3D unit sphere; returns labels 0..npatch-1\"\"\"\n",
    "    xyz = sphere_to_cartesian(ra_deg, dec_deg)\n",
    "    kmeans = KMeans(n_clusters=npatch, random_state=random_state, n_init=10)\n",
    "    labels = kmeans.fit_predict(xyz)\n",
    "    return labels\n",
    "\n",
    "\n",
    "def make_patches_healpix(ra_deg, dec_deg, npatch):\n",
    "    \"\"\"Make patches by grouping healpix pixels. Requires healpy.\n",
    "    We choose nside such that number of pixels ~ npatch, then group nearby pixels\n",
    "    to form ~npatch patches. This is a simple heuristic and might be adapted.\n",
    "    \"\"\"\n",
    "    if not HAS_HEALPY:\n",
    "        raise RuntimeError(\"healpy not available\")\n",
    "    # choose nside with npix close to npatch\n",
    "    target_npixels = npatch\n",
    "    nside = 1\n",
    "    while hp.nside2npix(nside) < target_npixels:\n",
    "        nside *= 2\n",
    "    pix = hp.ang2pix(nside, np.deg2rad(90-dec_deg), np.deg2rad(ra_deg), lonlat=False)\n",
    "    # group contiguous pixels into patches by simple modular binning\n",
    "    # (this is intentionally simple; user may want to replace with better grouping)\n",
    "    uniq = np.unique(pix)\n",
    "    # map pixel -> group by hashing\n",
    "    pix_to_group = {p: (i % npatch) for i, p in enumerate(uniq)}\n",
    "    labels = np.array([pix_to_group[p] for p in pix])\n",
    "    return labels\n",
    "\n",
    "# ----------------------------- Diagnostics -----------------------------\n",
    "\n",
    "def patch_counts_from_labels(labels, npatch=None, weights=None):\n",
    "    if npatch is None:\n",
    "        npatch = labels.max()+1\n",
    "    counts = np.bincount(labels, minlength=npatch)\n",
    "    if weights is None:\n",
    "        return counts\n",
    "    # weighted counts: sum weights per patch\n",
    "    w = np.zeros(npatch, dtype=float)\n",
    "    for lab, ww in zip(labels, weights):\n",
    "        w[lab] += ww\n",
    "    return w\n",
    "\n",
    "\n",
    "def patch_pair_counts(counts):\n",
    "    \"\"\"Approximate number of unique pairs inside each patch using n*(n-1)/2.\n",
    "    For weighted samples this is only an approximation (could use sum_w*(sum_w-1)/2).\n",
    "    Returns array of pair counts per patch.\n",
    "    \"\"\"\n",
    "    n = np.asarray(counts)\n",
    "    pairs = n*(n-1)/2\n",
    "    return pairs\n",
    "\n",
    "\n",
    "def patch_angular_radius_deg(labels, ra_deg, dec_deg, npatch=None):\n",
    "    \"\"\"Estimate patch angular radius (max angular separation from patch centroid) in degrees.\n",
    "    This is a conservative estimate of patch size. If a patch has 1 object, radius=0.\n",
    "    \"\"\"\n",
    "    if npatch is None:\n",
    "        npatch = labels.max()+1\n",
    "    radii = np.zeros(npatch, dtype=float)\n",
    "    for p in range(npatch):\n",
    "        mask = (labels == p)\n",
    "        if mask.sum() == 0:\n",
    "            radii[p] = 0.0\n",
    "            continue\n",
    "        ras = ra_deg[mask]\n",
    "        decs = dec_deg[mask]\n",
    "        # centroid in cartesian, then to (ra,dec)\n",
    "        xyz = sphere_to_cartesian(ras, decs)\n",
    "        centroid = xyz.mean(axis=0)\n",
    "        # normalize centroid\n",
    "        centroid = centroid / np.linalg.norm(centroid)\n",
    "        # convert centroid to ra,dec\n",
    "        cx, cy, cz = centroid\n",
    "        dec_c = np.degrees(np.arcsin(cz))\n",
    "        ra_c = np.degrees(np.arctan2(cy, cx)) % 360.0\n",
    "        # compute max angular separation\n",
    "        sep = angular_separation_deg(ra_c, dec_c, ras, decs)\n",
    "        radii[p] = sep.max() if sep.size>0 else 0.0\n",
    "    return radii\n",
    "\n",
    "\n",
    "def kish_neff_from_counts(counts):\n",
    "    counts = np.asarray(counts, dtype=float)\n",
    "    s = counts.sum()\n",
    "    s2 = (counts**2).sum()\n",
    "    if s2 <= 0:\n",
    "        return 0.0\n",
    "    return (s*s)/s2\n",
    "\n",
    "\n",
    "def patch_statistics(labels, ra_deg, dec_deg, weights=None, npatch=None):\n",
    "    if npatch is None:\n",
    "        npatch = labels.max()+1\n",
    "    counts = patch_counts_from_labels(labels, npatch=npatch, weights=None if weights is None else weights)\n",
    "    pairs = patch_pair_counts(counts)\n",
    "    radii = patch_angular_radius_deg(labels, ra_deg, dec_deg, npatch=npatch)\n",
    "\n",
    "    stats = {\n",
    "        'npatch': int(npatch),\n",
    "        'counts_min': int(counts.min()),\n",
    "        'counts_median': float(np.median(counts)),\n",
    "        'counts_mean': float(np.mean(counts)),\n",
    "        'counts_std': float(np.std(counts, ddof=1)) if len(counts)>1 else 0.0,\n",
    "        'cv': float(np.std(counts, ddof=1)/np.mean(counts)) if np.mean(counts)>0 else float('inf'),\n",
    "        'frac_empty': float((counts==0).sum()/len(counts)),\n",
    "        'pairs_min': float(pairs.min()),\n",
    "        'pairs_median': float(np.median(pairs)),\n",
    "        'pairs_mean': float(np.mean(pairs)),\n",
    "        'kish_neff': float(kish_neff_from_counts(counts)),\n",
    "        'patch_radius_deg_mean': float(radii.mean()),\n",
    "        'patch_radius_deg_median': float(np.median(radii)),\n",
    "        'patch_radius_deg_min': float(radii.min()),\n",
    "        'patch_radius_deg_max': float(radii.max()),\n",
    "    }\n",
    "    return stats\n",
    "\n",
    "# ----------------------------- Decision Logic -----------------------------\n",
    "\n",
    "def choose_patches_and_resampling(ra_deg, dec_deg,\n",
    "                                  npatch_candidates=None,\n",
    "                                  method='kmeans',\n",
    "                                  weights=None,\n",
    "                                  # thresholds tuned for galaxy 2-pt work; adapt as needed\n",
    "                                  min_per_patch=20,\n",
    "                                  median_per_patch=30,\n",
    "                                  frac_empty_max=0.10,\n",
    "                                  pairs_min_per_patch=50,\n",
    "                                  kish_neff_min=15,\n",
    "                                  max_correlation_scale_deg=2.0,\n",
    "                                  cv_warning=3.0,\n",
    "                                  random_state=0):\n",
    "    \"\"\"Decide whether patch resampling is valid and recommend npatch & method.\n",
    "\n",
    "    Key points of logic (aligned with user's request):\n",
    "      - CV is only used as a WARNING (not a strict reject) unless it tracks emptiness.\n",
    "      - Reject npatch if any of these are true:\n",
    "         * min_per_patch not met\n",
    "         * too many empty patches (frac_empty > frac_empty_max)\n",
    "         * too few pairs per patch on average (pairs_min_per_patch)\n",
    "         * kish neff too small\n",
    "         * patch angular radius too small compared to the max correlation scale\n",
    "\n",
    "    Returns dict with decision, stats per candidate, chosen labels when available, and warnings.\n",
    "    \"\"\"\n",
    "\n",
    "    N = len(ra_deg)\n",
    "    if npatch_candidates is None:\n",
    "        # reasonable defaults: try a few values including small and large patch counts\n",
    "        upper = max(10, min(500, max(10, N//10)))\n",
    "        npatch_candidates = np.unique(np.concatenate([\n",
    "            np.linspace(10, upper, num=8, dtype=int),\n",
    "            np.array([20, 30, 50, 100, 200], dtype=int)\n",
    "        ]))\n",
    "\n",
    "    labels_cache = {}\n",
    "    stats_cache = {}\n",
    "    accepted = []\n",
    "    warnings = defaultdict(list)\n",
    "\n",
    "    for npatch in npatch_candidates:\n",
    "        if npatch <= 1:\n",
    "            continue\n",
    "        # create labels\n",
    "        if method == 'kmeans':\n",
    "            labels = make_patches_kmeans(ra_deg, dec_deg, npatch, random_state=random_state)\n",
    "        elif method == 'healpix':\n",
    "            labels = make_patches_healpix(ra_deg, dec_deg, npatch)\n",
    "        else:\n",
    "            raise ValueError('unknown method')\n",
    "        labels_cache[npatch] = labels\n",
    "        st = patch_statistics(labels, ra_deg, dec_deg)\n",
    "        stats_cache[npatch] = st\n",
    "\n",
    "        # compute rejection conditions\n",
    "        reject_reasons = []\n",
    "        if st['counts_min'] < min_per_patch:\n",
    "            reject_reasons.append(f\"min_per_patch={st['counts_min']}<{min_per_patch}\")\n",
    "        if st['frac_empty'] > frac_empty_max:\n",
    "            reject_reasons.append(f\"frac_empty={st['frac_empty']:.3f}>{frac_empty_max}\")\n",
    "        if st['pairs_median'] < pairs_min_per_patch:\n",
    "            reject_reasons.append(f\"median_pairs={st['pairs_median']:.1f}<{pairs_min_per_patch}\")\n",
    "        if st['kish_neff'] < kish_neff_min:\n",
    "            reject_reasons.append(f\"kish_neff={st['kish_neff']:.1f}<{kish_neff_min}\")\n",
    "        # require patch radius to be at least the max correlation scale (preferably larger)\n",
    "        if st['patch_radius_deg_median'] < max_correlation_scale_deg*0.5:\n",
    "            # This is a softer check (0.5 factor) because patches are allowed to be smaller but ideally bigger\n",
    "            reject_reasons.append(f\"median_patch_radius={st['patch_radius_deg_median']:.2f}deg<~{max_correlation_scale_deg*0.5}deg\")\n",
    "\n",
    "        # CV only as a warning unless it's caused by emptiness (then already in reject reasons)\n",
    "        if st['cv'] > cv_warning:\n",
    "            warnings[npatch].append(f\"High CV={st['cv']:.2f} (warning only) -- may reflect real LSS\")\n",
    "\n",
    "        if reject_reasons:\n",
    "            warnings[npatch].extend(reject_reasons)\n",
    "        else:\n",
    "            accepted.append(npatch)\n",
    "\n",
    "    decision = {\n",
    "        'accepted': bool(accepted),\n",
    "        'accepted_npatches': sorted(accepted),\n",
    "        'chosen_npatch': max(accepted) if accepted else None,\n",
    "        'method': method,\n",
    "        'stats': stats_cache,\n",
    "        'labels': labels_cache,\n",
    "        'warnings': dict(warnings),\n",
    "        'recommendation': None,\n",
    "    }\n",
    "\n",
    "    if not accepted:\n",
    "        decision['recommendation'] = 'point_bootstrap'\n",
    "        return decision\n",
    "\n",
    "    # choose largest accepted npatch by default\n",
    "    chosen = decision['chosen_npatch']\n",
    "    decision['recommendation'] = 'patch_jackknife' if chosen >= 30 else 'patch_bootstrap'\n",
    "\n",
    "    # If CV warnings exist for the chosen npatch, preserve as an advisory note\n",
    "    if chosen in warnings and any('High CV' in w for w in warnings[chosen]):\n",
    "        decision['warnings_for_choice'] = warnings[chosen]\n",
    "    else:\n",
    "        decision['warnings_for_choice'] = []\n",
    "\n",
    "    # final note: return labels for chosen npatch for downstream jackknife\n",
    "    decision['chosen_labels'] = labels_cache[chosen]\n",
    "\n",
    "    return decision\n",
    "\n",
    "# ----------------------------- Example use -----------------------------\n",
    "if __name__ == '__main__':\n",
    "    # small test with toy data\n",
    "    import numpy as np\n",
    "    rng = np.random.default_rng(123)\n",
    "    # make an inhomogeneous toy survey: two overdensities + background\n",
    "    N = 5000\n",
    "    # background\n",
    "    ra_bg = rng.uniform(0, 360, size=int(0.8*N))\n",
    "    dec_bg = rng.uniform(-30, 30, size=int(0.8*N))\n",
    "    # overdensity 1\n",
    "    ra1 = rng.normal(180, 1, size=int(0.1*N))\n",
    "    dec1 = rng.normal(0, 0.5, size=int(0.1*N))\n",
    "    # overdensity 2\n",
    "    ra2 = rng.normal(60, 0.8, size=int(0.1*N))\n",
    "    dec2 = rng.normal(10, 0.4, size=int(0.1*N))\n",
    "    ra = np.concatenate([ra_bg, ra1, ra2])\n",
    "    dec = np.concatenate([dec_bg, dec1, dec2])\n",
    "\n",
    "    decision = choose_patches_and_resampling(ra, dec, npatch_candidates=[20,50,100,200], random_state=1)\n",
    "    print('Decision:')\n",
    "    print(decision['recommendation'], 'chosen', decision['chosen_npatch'])\n",
    "    print('Warnings (sample):', list(decision['warnings'].items())[:3])\n",
    "\n",
    "\n",
    "# End of file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading GLADE...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6096/1525763123.py:46: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  for chunk in pd.read_csv(filename, delim_whitespace=True, names=header,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Loaded 648 galaxies after cuts.\n",
      "Generating random catalog...\n",
      "\n",
      "Testing npatch=20\n",
      "  min=13, median=33.0, mean=32.4, CV=0.35, empty=0.00, KishNeff=17.8\n",
      "  -> patches too sparse (<20 galaxies)\n",
      "\n",
      "Testing npatch=30\n",
      "  min=9, median=19.5, mean=21.6, CV=0.44, empty=0.00, KishNeff=25.2\n",
      "  -> patches too sparse (<20 galaxies)\n",
      "\n",
      "Testing npatch=40\n",
      "  min=3, median=14.0, mean=16.2, CV=0.47, empty=0.00, KishNeff=32.9\n",
      "  -> patches too sparse (<20 galaxies)\n",
      "\n",
      "Testing npatch=50\n",
      "  min=2, median=11.0, mean=13.0, CV=0.57, empty=0.00, KishNeff=37.7\n",
      "  -> patches too sparse (<20 galaxies)\n",
      "\n",
      "Testing npatch=60\n",
      "  min=2, median=10.0, mean=10.8, CV=0.52, empty=0.00, KishNeff=47.2\n",
      "  -> patches too sparse (<20 galaxies)\n",
      "\n",
      "Testing npatch=80\n",
      "  min=2, median=8.0, mean=8.1, CV=0.52, empty=0.00, KishNeff=62.7\n",
      "  -> patches too sparse (<20 galaxies)\n",
      "\n",
      "Testing npatch=100\n",
      "  min=1, median=5.5, mean=6.5, CV=0.62, empty=0.00, KishNeff=72.0\n",
      "  -> patches too sparse (<20 galaxies)\n",
      "\n",
      "Testing npatch=150\n",
      "  min=1, median=4.0, mean=4.4, CV=0.69, empty=0.00, KishNeff=100.6\n",
      "  -> patches too sparse (<20 galaxies)\n",
      "\n",
      ">>> No valid patching found. Switching to MANUAL point-by-point bootstrap.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Parameter var_method has invalid value manual.  Valid values are ['shot', 'jackknife', 'sample', 'bootstrap', 'marked_bootstrap'].",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 243\u001b[39m\n\u001b[32m    237\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m nk\n\u001b[32m    240\u001b[39m \u001b[38;5;66;03m# ------------------------------------------------\u001b[39;00m\n\u001b[32m    241\u001b[39m \u001b[38;5;66;03m# Execute\u001b[39;00m\n\u001b[32m    242\u001b[39m \u001b[38;5;66;03m# ------------------------------------------------\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m243\u001b[39m nk = \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 234\u001b[39m, in \u001b[36mpipeline\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    229\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m>>> Selected npatch=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_npatch\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m using JACKKNIFE resampling\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    231\u001b[39m \u001b[38;5;66;03m# ----------------------------------------------------------------\u001b[39;00m\n\u001b[32m    232\u001b[39m \u001b[38;5;66;03m# Run TreeCorr\u001b[39;00m\n\u001b[32m    233\u001b[39m \u001b[38;5;66;03m# ----------------------------------------------------------------\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m234\u001b[39m nk = \u001b[43mrun_treecorr_NK\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgxs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mra_rand\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdec_rand\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresampling_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnpatch\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbest_npatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    236\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mDone.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    237\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m nk\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 171\u001b[39m, in \u001b[36mrun_treecorr_NK\u001b[39m\u001b[34m(gxs, r_ra, r_dec, resampling_mode, npatch)\u001b[39m\n\u001b[32m    168\u001b[39m config = \u001b[38;5;28mdict\u001b[39m(TC_CONFIG)\n\u001b[32m    169\u001b[39m config[\u001b[33m\"\u001b[39m\u001b[33mvar_method\u001b[39m\u001b[33m\"\u001b[39m] = resampling_mode\n\u001b[32m--> \u001b[39m\u001b[32m171\u001b[39m nk = \u001b[43mtreecorr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mNKCorrelation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    172\u001b[39m nk.process(cat_gal, cat_rnd)\n\u001b[32m    174\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mComputed NK with resampling=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresampling_mode\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, npatch=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnpatch\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/auger/lib/python3.13/site-packages/treecorr/nkcorrelation.py:90\u001b[39m, in \u001b[36mNKCorrelation.__init__\u001b[39m\u001b[34m(self, config, logger, **kwargs)\u001b[39m\n\u001b[32m     89\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, config=\u001b[38;5;28;01mNone\u001b[39;00m, *, logger=\u001b[38;5;28;01mNone\u001b[39;00m, **kwargs):\n\u001b[32m---> \u001b[39m\u001b[32m90\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogger\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlogger\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     92\u001b[39m     \u001b[38;5;28mself\u001b[39m._xi[\u001b[32m0\u001b[39m] = np.zeros_like(\u001b[38;5;28mself\u001b[39m.rnom, dtype=\u001b[38;5;28mfloat\u001b[39m)\n\u001b[32m     93\u001b[39m     \u001b[38;5;28mself\u001b[39m.xi = \u001b[38;5;28mself\u001b[39m.raw_xi\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/auger/lib/python3.13/site-packages/treecorr/corr2base.py:348\u001b[39m, in \u001b[36mCorr2.__init__\u001b[39m\u001b[34m(self, config, logger, rng, **kwargs)\u001b[39m\n\u001b[32m    346\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, config=\u001b[38;5;28;01mNone\u001b[39;00m, *, logger=\u001b[38;5;28;01mNone\u001b[39;00m, rng=\u001b[38;5;28;01mNone\u001b[39;00m, **kwargs):\n\u001b[32m    347\u001b[39m     \u001b[38;5;28mself\u001b[39m._corr = \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# Do this first to make sure we always have it for __del__\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m348\u001b[39m     \u001b[38;5;28mself\u001b[39m.config = \u001b[43mmerge_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43mCorr2\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_valid_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    349\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m logger \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    350\u001b[39m         \u001b[38;5;28mself\u001b[39m._logger_name = \u001b[33m'\u001b[39m\u001b[33mtreecorr.Corr2\u001b[39m\u001b[33m'\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/auger/lib/python3.13/site-packages/treecorr/config.py:420\u001b[39m, in \u001b[36mmerge_config\u001b[39m\u001b[34m(config, kwargs, valid_params, aliases)\u001b[39m\n\u001b[32m    418\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m valid_params \u001b[38;5;129;01mand\u001b[39;00m key \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m kwargs:\n\u001b[32m    419\u001b[39m             kwargs[key] = value\n\u001b[32m--> \u001b[39m\u001b[32m420\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcheck_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maliases\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/auger/lib/python3.13/site-packages/treecorr/config.py:274\u001b[39m, in \u001b[36mcheck_config\u001b[39m\u001b[34m(config, params, aliases, logger)\u001b[39m\n\u001b[32m    272\u001b[39m         matches = [v \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m valid_values \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(v,\u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m value.startswith(v)]\n\u001b[32m    273\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(matches) != \u001b[32m1\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m274\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mParameter \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m has invalid value \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m.  Valid values are \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m%(\n\u001b[32m    275\u001b[39m             key, config[key], \u001b[38;5;28mstr\u001b[39m(valid_values)))\n\u001b[32m    276\u001b[39m     value = matches[\u001b[32m0\u001b[39m]\n\u001b[32m    277\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mValueError\u001b[39m: Parameter var_method has invalid value manual.  Valid values are ['shot', 'jackknife', 'sample', 'bootstrap', 'marked_bootstrap']."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import healpy as hp\n",
    "import treecorr\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ------------------------------------------------\n",
    "# User parameters (from your snippet)\n",
    "# ------------------------------------------------\n",
    "GLADE_FILE = \"../../data/GLADE_zhelio_lt0.1_dL_lt300.txt\"\n",
    "DEFL_FILE  = \"../../data/deflexionZ2E510_R2_ra_dec_deg.txt\"\n",
    "FLUX_FILE  = \"../../data/Auger/flux_a8_lb.dat\"\n",
    "\n",
    "PLOTS_DIR = \"../../plots\"\n",
    "\n",
    "FWHM_DEG = 30.0\n",
    "N_RAND_PER_GAL = 100\n",
    "MIN_CZ = 1200\n",
    "MAX_CZ = 4000\n",
    "MK_CUT = -20.0\n",
    "MAX_DEC = 45.0\n",
    "\n",
    "TC_CONFIG = dict(\n",
    "    min_sep=1,\n",
    "    max_sep=65.,\n",
    "    nbins=13,\n",
    "    sep_units='degree',\n",
    "    bin_type='Linear',\n",
    "    brute=False,\n",
    "    metric='Arc',\n",
    ")\n",
    "\n",
    "# ------------------------------------------------\n",
    "# Helper functions (from your provided code)\n",
    "# ------------------------------------------------\n",
    "\n",
    "def read_GLADE(cols_to_use, cz_min=MIN_CZ, dL_max=200, filename=GLADE_FILE):\n",
    "    from get_masks import get_milkyway_mask\n",
    "    c = 299792.458\n",
    "\n",
    "    chunksize = 200_000\n",
    "    with open(filename, 'r') as f:\n",
    "        header = f.readline().lstrip('#').split()\n",
    "\n",
    "    chunks = []\n",
    "    for chunk in pd.read_csv(filename, delim_whitespace=True, names=header,\n",
    "                             usecols=cols_to_use, skiprows=1, chunksize=chunksize):\n",
    "        # Apply filters\n",
    "        chunk = chunk[chunk[\"dist_flag\"] != 0]\n",
    "        chunk[\"cz\"] = c * chunk[\"z_cmb\"]\n",
    "        chunk = chunk[(chunk[\"cz\"] > cz_min) & (chunk[\"d_L\"] < dL_max)]\n",
    "        chunk[\"M_K\"] = chunk[\"K\"] - 5*np.log10(chunk[\"d_L\"]) - 25\n",
    "\n",
    "        mask = np.isfinite(chunk[\"M_K\"]) & np.isfinite(chunk[\"d_L\"])\n",
    "        mask &= np.isfinite(chunk[\"RA\"]) & np.isfinite(chunk[\"Dec\"])\n",
    "        chunks.append(chunk.loc[mask])\n",
    "\n",
    "    df = pd.concat(chunks, ignore_index=True)\n",
    "\n",
    "    from get_masks import get_milkyway_mask\n",
    "    df = df.iloc[get_milkyway_mask(df[\"RA\"].values, df[\"Dec\"].values)]\n",
    "    df = df[df[\"Dec\"] < MAX_DEC]\n",
    "    return df\n",
    "\n",
    "\n",
    "def generate_RanCat(N_total, dec_min=-90, dec_max=MAX_DEC):\n",
    "    from get_masks import get_milkyway_mask\n",
    "\n",
    "    # Generate in sin(dec)\n",
    "    ra = np.random.uniform(0, 360, 3*N_total)\n",
    "    sindec = np.random.uniform(np.sin(np.radians(dec_min)),\n",
    "                               np.sin(np.radians(dec_max)),\n",
    "                               3*N_total)\n",
    "    dec = np.degrees(np.arcsin(sindec))\n",
    "\n",
    "    mask = get_milkyway_mask(ra, dec)\n",
    "    ra = ra[mask][:N_total]\n",
    "    dec = dec[mask][:N_total]\n",
    "\n",
    "    if len(ra) < N_total:\n",
    "        raise ValueError(\"Not enough randoms after masking!\")\n",
    "\n",
    "    return ra, dec\n",
    "\n",
    "\n",
    "# ------------------------------------------------\n",
    "# Patch diagnostics\n",
    "# ------------------------------------------------\n",
    "\n",
    "def compute_patch_stats(patch_ids):\n",
    "    \"\"\"Return statistics for patch occupancy.\"\"\"\n",
    "    values, counts = np.unique(patch_ids, return_counts=True)\n",
    "    num_patches = values.size\n",
    "\n",
    "    min_ct = counts.min()\n",
    "    frac_empty = np.mean(counts == 0)\n",
    "    cv = counts.std() / counts.mean()\n",
    "\n",
    "    kish_neff = (counts.sum())**2 / (counts**2).sum()\n",
    "\n",
    "    info = dict(\n",
    "        num_patches=num_patches,\n",
    "        min=min_ct,\n",
    "        median=np.median(counts),\n",
    "        mean=np.mean(counts),\n",
    "        std=np.std(counts),\n",
    "        cv=cv,\n",
    "        frac_empty=frac_empty,\n",
    "        kish_neff=kish_neff,\n",
    "    )\n",
    "    return info, counts\n",
    "\n",
    "\n",
    "def passes_thresholds(stats, Nbins):\n",
    "    \"\"\"\n",
    "    Main logic for deciding if patching is usable.\n",
    "    NEW criteria based on our theoretical discussion.\n",
    "    \"\"\"\n",
    "    if stats[\"frac_empty\"] > 0.10:\n",
    "        return False, \"too many empty patches\"\n",
    "\n",
    "    if stats[\"min\"] < 20:\n",
    "        return False, \"patches too sparse (<20 galaxies)\"\n",
    "\n",
    "    if stats[\"kish_neff\"] < 2 * Nbins:\n",
    "        return False, \"effective patch number too small\"\n",
    "\n",
    "    # CV only used for extreme cases\n",
    "    if stats[\"cv\"] > 2.5 and stats[\"min\"] < 50:\n",
    "        return False, \"CV very high AND patch occupancy low\"\n",
    "\n",
    "    return True, \"OK\"\n",
    "\n",
    "\n",
    "# ------------------------------------------------\n",
    "# Run NK correlation with choice of resampling strategy\n",
    "# ------------------------------------------------\n",
    "\n",
    "def run_treecorr_NK(gxs, r_ra, r_dec, resampling_mode, npatch=None):\n",
    "    import treecorr\n",
    "\n",
    "    # galaxy catalog\n",
    "    if npatch is None:\n",
    "        cat_gal = treecorr.Catalog(\n",
    "            ra=gxs[\"RA\"], dec=gxs[\"Dec\"],\n",
    "            ra_units=\"degree\", dec_units=\"degree\",\n",
    "            k=np.ones(len(gxs)),\n",
    "        )\n",
    "        cat_rnd = treecorr.Catalog(\n",
    "            ra=r_ra, dec=r_dec,\n",
    "            ra_units=\"degree\", dec_units=\"degree\",\n",
    "            k=np.ones(len(r_ra)),\n",
    "        )\n",
    "    else:\n",
    "        cat_gal = treecorr.Catalog(\n",
    "            ra=gxs[\"RA\"], dec=gxs[\"Dec\"],\n",
    "            ra_units=\"degree\", dec_units=\"degree\",\n",
    "            k=np.ones(len(gxs)),\n",
    "            npatch=npatch\n",
    "        )\n",
    "        cat_rnd = treecorr.Catalog(\n",
    "            ra=r_ra, dec=r_dec,\n",
    "            ra_units=\"degree\", dec_units=\"degree\",\n",
    "            k=np.ones(len(r_ra)),\n",
    "            npatch=npatch\n",
    "        )\n",
    "\n",
    "    config = dict(TC_CONFIG)\n",
    "    config[\"var_method\"] = resampling_mode\n",
    "\n",
    "    nk = treecorr.NKCorrelation(config=config)\n",
    "    nk.process(cat_gal, cat_rnd)\n",
    "\n",
    "    print(f\"Computed NK with resampling={resampling_mode}, npatch={npatch}\")\n",
    "    print(\"Cov shape:\", None if nk.cov is None else nk.cov.shape)\n",
    "    return nk\n",
    "\n",
    "\n",
    "# ------------------------------------------------\n",
    "# Master pipeline\n",
    "# ------------------------------------------------\n",
    "\n",
    "def pipeline():\n",
    "    # ---- load data ----\n",
    "    print(\"Loading GLADE...\")\n",
    "    gxs = read_GLADE([\"RA\", \"Dec\", \"z_cmb\", \"K\", \"d_L\", \"dist_flag\"])\n",
    "    gxs = gxs[(gxs['cz'] < MAX_CZ) & (gxs['cz'] > MIN_CZ) &\\\n",
    "              (gxs['M_K'] < -24.) & (gxs['Dec'] < MAX_DEC)]\n",
    "    print(f\"  Loaded {len(gxs)} galaxies after cuts.\")\n",
    "    \n",
    "    # Randoms\n",
    "    print(\"Generating random catalog...\")\n",
    "    ra_rand, dec_rand = generate_RanCat(N_RAND_PER_GAL * len(gxs))\n",
    "\n",
    "    # ---- try candidate patch numbers ----\n",
    "    candidates = [20, 30, 40, 50, 60, 80, 100, 150]\n",
    "\n",
    "    good_candidates = []\n",
    "\n",
    "    for npatch in candidates:\n",
    "        print(f\"\\nTesting npatch={npatch}\")\n",
    "        cat = treecorr.Catalog(\n",
    "            ra=gxs[\"RA\"], dec=gxs[\"Dec\"],\n",
    "            ra_units=\"degree\", dec_units=\"degree\",\n",
    "            npatch=npatch\n",
    "        )\n",
    "        stats, counts = compute_patch_stats(cat.patch)\n",
    "        ok, reason = passes_thresholds(stats, Nbins=TC_CONFIG[\"nbins\"])\n",
    "\n",
    "        print(f\"  min={stats['min']}, median={stats['median']:.1f}, \"\n",
    "              f\"mean={stats['mean']:.1f}, CV={stats['cv']:.2f}, \"\n",
    "              f\"empty={stats['frac_empty']:.2f}, KishNeff={stats['kish_neff']:.1f}\")\n",
    "        print(f\"  -> {reason}\")\n",
    "\n",
    "        if ok:\n",
    "            good_candidates.append((npatch, stats))\n",
    "\n",
    "    # ----------------------------------------------------------------\n",
    "    # Decide the mode\n",
    "    # ----------------------------------------------------------------\n",
    "    if len(good_candidates) == 0:\n",
    "        print(\"\\n>>> No valid patching found. Switching to MANUAL point-by-point bootstrap.\")\n",
    "        mode = \"manual\"\n",
    "        best_npatch = None\n",
    "    else:\n",
    "        # Choose the npatch with largest Kish effective number\n",
    "        best_npatch = max(good_candidates, key=lambda t: t[1][\"kish_neff\"])[0]\n",
    "        mode = \"jackknife\"\n",
    "        print(f\"\\n>>> Selected npatch={best_npatch} using JACKKNIFE resampling\")\n",
    "\n",
    "    # ----------------------------------------------------------------\n",
    "    # Run TreeCorr\n",
    "    # ----------------------------------------------------------------\n",
    "    nk = run_treecorr_NK(gxs, ra_rand, dec_rand, resampling_mode=mode, npatch=best_npatch)\n",
    "\n",
    "    print(\"\\nDone.\")\n",
    "    return nk\n",
    "\n",
    "\n",
    "# ------------------------------------------------\n",
    "# Execute\n",
    "# ------------------------------------------------\n",
    "nk = pipeline()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "auger",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
